{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-summarisation-ProphetNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM6UwYtlVbS8qfFJppTS3bm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umerhasan17/NLPzoo/blob/master/text_summarisation_ProphetNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SopYa7V56j39",
        "colab_type": "text"
      },
      "source": [
        "# ProphetNet\n",
        "\n",
        "ProphetNet is a Seq2Seq pre-training model`. The model is optimized by **n-step ahead prediction** aiming to predict the next n tokens simultaneously. This is an alternative to the traditional **1-step** Seq2Seq model. Modelling for n-steps prevents overfitting on 'strong local correlations' by encouraging planning for future tokens.\n",
        "\n",
        "[paper](https://arxiv.org/pdf/2001.04063.pdf) &nbsp;&nbsp;&nbsp;&nbsp;\n",
        "[code](https://github.com/microsoft/ProphetNet)\n",
        "\n",
        "## What are pre-trained models?\n",
        "\n",
        "A model that is trained to solve a problem similar to the one we want to solve on a large benchmark dataset. This can take the form of labeled data or unlabeled data with 'specific self-supervised objectives'. Pre-trained models are then 'fine-tuned to adapt to downstream tasks.'\n",
        "\n",
        "**Key terms to investigate:** Autoregressive language modelling, teacher forcing, bigram combination, greedy decoding, beam search.\n",
        "\n",
        "## Implementation Details\n",
        "\n",
        "There are 4 elements: \n",
        "  * the objective\n",
        "  * the n-stream self-attention mechanism\n",
        "  * modified positional embeddings\n",
        "  * mask based auto-encoder denoising"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8E6LgsCd54PL",
        "colab_type": "code",
        "outputId": "e48dde1a-0dbf-46b8-ac17-3ae16e23c198",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# Imports\n",
        "!pip install fairseq\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from multiprocessing import Pool\n",
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Parameter\n",
        "\n",
        "from fairseq import options, utils\n",
        "from fairseq.models import (\n",
        "    FairseqEncoder,\n",
        "    FairseqIncrementalDecoder,\n",
        "    FairseqEncoderDecoderModel,\n",
        "    register_model,\n",
        "    register_model_architecture,\n",
        ")\n",
        "from fairseq.modules import (\n",
        "    MultiheadAttention,\n",
        "    LayerNorm,\n",
        ")\n",
        "from fairseq.modules.transformer_sentence_encoder import init_bert_params\n",
        "\n",
        "from fairseq.tokenizer import tokenize_line\n",
        "from fairseq.binarizer import safe_readline\n",
        "from fairseq.data import data_utils, Dictionary\n",
        "from fairseq.criterions import FairseqCriterion, register_criterion \n",
        "from fairseq.tasks import register_task\n",
        "from fairseq.tasks.translation import TranslationTask\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fairseq in /usr/local/lib/python3.6/dist-packages (0.9.0)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.6/dist-packages (from fairseq) (0.29.19)\n",
            "Requirement already satisfied: cffi in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.14.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.18.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.5.0+cu101)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.6/dist-packages (from fairseq) (1.4.10)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from fairseq) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from fairseq) (4.41.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi->fairseq) (2.20)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->fairseq) (0.16.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.6/dist-packages (from sacrebleu->fairseq) (1.7.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozC2CgChVSJV",
        "colab_type": "text"
      },
      "source": [
        "**The Objective**\n",
        "\n",
        "* The encoder side is the same as the original transformer. \n",
        "* The decoder predicts the next n tokens instead of just 1.\n",
        "* Weights are assigned to loss values for all streams (giving higher weight to closer tokens is similar to the discount factor of future reward in reinforcement learning).\n",
        "\n",
        "**TODO: what does this mean?**\n",
        "\n",
        "\"where the decoder outputs N probability at each time step.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPi_w1bVVTZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owyQc4iRWP-B",
        "colab_type": "text"
      },
      "source": [
        "**N-Stream Self-Attention Mechanism**\n",
        "\n",
        "* ProphetNet contains **n-stream self-attention** models with the main stream being the same as the self-attention in the original Transformer. \n",
        "* The parameters of the main stream are shared with every other predicting stream, this allows us to disable the n-stream during inference and only predict the next token. \n",
        "\n",
        "**Modified positional embedding**\n",
        "\n",
        "**Mask based auto-encoder denoising task for Seq2Seq pre-training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FufgpG9Df2Mn",
        "colab_type": "text"
      },
      "source": [
        "# Translation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9eBsk2Ef1jc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#from fairseq.data import BertDictionary\n",
        "\n",
        "@register_task('translation_prophetnet')\n",
        "class TranslationProphetnetTask(TranslationTask):\n",
        "    def __init__(self, args, src_dict, tgt_dict):\n",
        "        super().__init__(args, src_dict, tgt_dict)\n",
        "\n",
        "    @classmethod\n",
        "    def load_dictionary(cls, filename):\n",
        "        return BertDictionary.load_from_file(filename)\n",
        "\n",
        "    def max_positions(self):\n",
        "        \"\"\"Return the max sentence length allowed by the task.\"\"\"\n",
        "        return (self.args.max_source_positions, self.args.max_target_positions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6asVHd0fn44",
        "colab_type": "text"
      },
      "source": [
        "# N-Gram Criterions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DXfSX7Cfd_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# This source code is licensed under the MIT license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "@register_criterion('ngram_language_loss')\n",
        "class NgramLmLoss(FairseqCriterion):\n",
        "    \"\"\"\n",
        "    Implementation for the loss used in masked language model (MLM) training.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, task):\n",
        "        super().__init__(args, task)\n",
        "        self.eps = args.label_smoothing\n",
        "        self.disable_ngram_loss = args.disable_ngram_loss\n",
        "\n",
        "    @staticmethod\n",
        "    def add_args(parser):\n",
        "        \"\"\"Add criterion-specific arguments to the parser.\"\"\"\n",
        "        # fmt: off\n",
        "        parser.add_argument('--label-smoothing', default=0., type=float, metavar='D',\n",
        "                            help='epsilon for label smoothing, 0 means no label smoothing')\n",
        "        parser.add_argument('--disable-ngram-loss', action='store_true',\n",
        "                            help='only comput basic stat')\n",
        "        # fmt: on\n",
        "\n",
        "    def forward(self, model, sample, reduce=True):\n",
        "        \"\"\"Compute the loss for the given sample.\n",
        "        Returns a tuple with three elements:\n",
        "        1) the loss\n",
        "        2) the sample size, which is used as the denominator for the gradient\n",
        "        3) logging outputs to display while training\n",
        "        \"\"\"\n",
        "        # compute MLM loss\n",
        "        logits_list = model(**sample['net_input'], return_all_hiddens=False)[0]\n",
        "        targets = model.get_targets(sample, [logits_list[0]])\n",
        "\n",
        "\n",
        "        ngram = len(logits_list)\n",
        "        # [B, ngram, T]\n",
        "        expend_targets = targets.new_zeros(ngram, targets.size(0), targets.size(1)).fill_(self.padding_idx)\n",
        "        for i in range(ngram):\n",
        "            if i > 0 and self.disable_ngram_loss:\n",
        "                break\n",
        "\n",
        "            padding_targets = torch.zeros_like(targets).fill_(self.padding_idx)\n",
        "            if 'target_idx' in sample:\n",
        "                expend_targets[i,:,:] = torch.where(sample['target_idx'] >= i, targets, padding_targets)\n",
        "            else:\n",
        "                expend_targets[i,:,:] = targets\n",
        "        targets = expend_targets\n",
        "\n",
        "        logits = torch.cat(logits_list, dim=0) #.view(ngram, *logits_list[0].size())\n",
        "\n",
        "        lprobs = F.log_softmax(\n",
        "                    logits.view(-1, logits.size(-1)),\n",
        "                    dim=-1,\n",
        "                    dtype=torch.float32,\n",
        "                )\n",
        "\n",
        "        loss = F.nll_loss(\n",
        "               lprobs,\n",
        "               targets.view(-1),\n",
        "               reduction='sum',\n",
        "               ignore_index=self.padding_idx,\n",
        "               )\n",
        "\n",
        "        if self.eps > 0.:\n",
        "            smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
        "            non_pad_mask = targets.ne(self.padding_idx).view(-1)\n",
        "            smooth_loss = smooth_loss[non_pad_mask]\n",
        "            smooth_loss = smooth_loss.sum()\n",
        "\n",
        "            eps_i = self.eps / lprobs.size(-1)\n",
        "            loss = (1. - self.eps) * loss + eps_i * smooth_loss\n",
        "\n",
        "        sample_size = targets.ne(self.padding_idx).int().sum().item()\n",
        "\n",
        "        logging_output = {\n",
        "            'loss': utils.item(loss.data) if reduce else loss.data,\n",
        "            'ntokens': sample['ntokens'],\n",
        "            'nsentences': sample['nsentences'],\n",
        "            'sample_size': sample_size,\n",
        "        }\n",
        "        return loss, sample_size, logging_output\n",
        "\n",
        "    @staticmethod\n",
        "    def aggregate_logging_outputs(logging_outputs):\n",
        "        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n",
        "        loss = sum(log.get('loss', 0) for log in logging_outputs)\n",
        "        ntokens = sum(log.get('ntokens', 0) for log in logging_outputs)\n",
        "        nsentences = sum(log.get('nsentences', 0) for log in logging_outputs)\n",
        "        sample_size = sum(log.get('sample_size', 0) for log in logging_outputs)\n",
        "\n",
        "        agg_output = {\n",
        "            'loss': loss / sample_size / math.log(2),\n",
        "            'ntokens': ntokens,\n",
        "            'nsentences': nsentences,\n",
        "            'sample_size': sample_size,\n",
        "        }\n",
        "        return agg_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBHWg2mmfTU_",
        "colab_type": "text"
      },
      "source": [
        "# BERT Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOAoXXmzfBKK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (c) Facebook, Inc. and its affiliates.\n",
        "#\n",
        "# This source code is licensed under the MIT license found in the\n",
        "# LICENSE file in the root directory of this source tree.\n",
        "\n",
        "class BertDictionary(Dictionary):\n",
        "    \"\"\"A mapping from symbols to consecutive integers\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        pad='<pad>',\n",
        "        eos='</s>',\n",
        "        unk='<unk>',\n",
        "        bos='<s>',\n",
        "        extra_special_symbols=None,\n",
        "    ):\n",
        "        super().__init__(pad, eos, unk, bos, extra_special_symbols)\n",
        "\n",
        "    @classmethod\n",
        "    def load_from_file(cls, filename):\n",
        "        d = cls()\n",
        "        d.symbols = []\n",
        "        d.count = []\n",
        "        d.indices = {}\n",
        "\n",
        "        with open(filename, 'r', encoding='utf-8', errors='ignore') as input_file:\n",
        "            for line in input_file:\n",
        "                k, v = line.split()\n",
        "                d.add_symbol(k)\n",
        "\n",
        "        d.unk_word = '[UNK]'\n",
        "        d.pad_word = '[PAD]'\n",
        "        d.eos_word = '[SEP]'\n",
        "        d.bos_word = '[CLS]'\n",
        "\n",
        "        d.bos_index = d.add_symbol('[CLS]')\n",
        "        d.pad_index = d.add_symbol('[PAD]')\n",
        "        d.eos_index = d.add_symbol('[SEP]')\n",
        "        d.unk_index = d.add_symbol('[UNK]')\n",
        "\n",
        "        d.nspecial = 999\n",
        "        return d\n",
        "\n",
        "    def save(self, f):\n",
        "        \"\"\"Stores dictionary into a text file\"\"\"\n",
        "        ex_keys, ex_vals = self._get_meta()\n",
        "        self._save(f, zip(ex_keys + self.symbols, ex_vals + self.count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwJSbwZ7boXk",
        "colab_type": "text"
      },
      "source": [
        "# Learned Positional Embedding Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "racmktBNhMsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LearnedPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"\n",
        "    This module learns positional embeddings up to a fixed maximum size.\n",
        "    Padding ids are ignored by either offsetting based on padding_idx\n",
        "    or by setting padding_idx to None and ensuring that the appropriate\n",
        "    position ids are passed to the forward function.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            num_embeddings: int,\n",
        "            embedding_dim: int,\n",
        "            padding_idx: int,\n",
        "    ):\n",
        "        super().__init__(num_embeddings, embedding_dim, padding_idx)\n",
        "        self.onnx_trace = False\n",
        "\n",
        "    def forward(self, input, incremental_state=None, positions=None):\n",
        "        \"\"\"Input is expected to be of size [bsz x seqlen].\"\"\"\n",
        "        assert (\n",
        "            (positions is None) or (self.padding_idx is None)\n",
        "        ), \"If positions is pre-computed then padding_idx should not be set.\"\n",
        "\n",
        "        if positions is None:\n",
        "            if incremental_state is not None:\n",
        "                # positions is the same for every token when decoding a single step\n",
        "                # Without the int() cast, it doesn't work in some cases when exporting to ONNX\n",
        "                positions = input.data.new(1, 1).fill_(int(self.padding_idx + input.size(1)))\n",
        "            else:\n",
        "                positions = utils.make_positions(\n",
        "                    input.data, self.padding_idx, onnx_trace=self.onnx_trace,\n",
        "                )\n",
        "            real_positions = positions\n",
        "        else:\n",
        "            real_positions = positions\n",
        "        return super().forward(positions), real_positions\n",
        "\n",
        "    def max_positions(self):\n",
        "        \"\"\"Maximum number of supported positions.\"\"\"\n",
        "        if self.padding_idx is not None:\n",
        "            return self.num_embeddings - self.padding_idx - 1\n",
        "        else:\n",
        "            return self.num_embeddings\n",
        "\n",
        "    def _forward(self, positions):\n",
        "        return super().forward(positions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLaE_x9Nar3S",
        "colab_type": "text"
      },
      "source": [
        "# Encoder Classes\n",
        "\n",
        "This includes the class for 1 layer and the class with multiple layers combined. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNDwJniRaBzT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a Transformer Encoder Layer used in BERT/XLM style pre-trained\n",
        "    models.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            embedding_dim: float = 768,\n",
        "            ffn_embedding_dim: float = 3072,\n",
        "            num_attention_heads: float = 8,\n",
        "            dropout: float = 0.1,\n",
        "            attention_dropout: float = 0.1,\n",
        "            activation_dropout: float = 0.1,\n",
        "            activation_fn: str = 'relu',\n",
        "            add_bias_kv: bool = False,\n",
        "            add_zero_attn: bool = False,\n",
        "            export: bool = False,\n",
        "    ) -> None:\n",
        "        super().__init__()\n",
        "        # Initialize parameters\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dropout = dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "\n",
        "        # Initialize blocks\n",
        "        self.activation_fn = utils.get_activation_fn(activation_fn)\n",
        "        self.self_attn = MultiheadAttention(\n",
        "            self.embedding_dim,\n",
        "            num_attention_heads,\n",
        "            dropout=attention_dropout,\n",
        "            add_bias_kv=add_bias_kv,\n",
        "            add_zero_attn=add_zero_attn,\n",
        "            self_attention=True,\n",
        "        )\n",
        "\n",
        "        # layer norm associated with the self attention layer\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)\n",
        "        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "        # layer norm associated with the position wise feed-forward NN\n",
        "        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x: torch.Tensor,\n",
        "            self_attn_mask: torch.Tensor = None,\n",
        "            self_attn_padding_mask: torch.Tensor = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        LayerNorm is applied either before or after the self-attention/ffn\n",
        "        modules similar to the original Transformer imlementation.\n",
        "        \"\"\"\n",
        "        residual = x\n",
        "        x, attn = self.self_attn(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            key_padding_mask=self_attn_padding_mask,\n",
        "            need_weights=False,\n",
        "            attn_mask=self_attn_mask,\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.final_layer_norm(x)\n",
        "        return x, attn\n",
        "\n",
        "\n",
        "class TransformerEncoder(FairseqEncoder):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *args.encoder_layers* layers. Each layer\n",
        "    is a :class:`TransformerEncoderLayer`.\n",
        "    Args:\n",
        "        args (argparse.Namespace): parsed command-line arguments\n",
        "        dictionary (~fairseq.data.Dictionary): encoding dictionary\n",
        "        embed_tokens (torch.nn.Embedding): input embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, dictionary, embed_tokens):\n",
        "        super().__init__(dictionary)\n",
        "        self.register_buffer('version', torch.Tensor([3]))\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "\n",
        "        embed_dim = embed_tokens.embedding_dim\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_source_positions = args.max_source_positions\n",
        "\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_scale = None #math.sqrt(embed_dim)\n",
        "        self.embed_positions = LearnedPositionalEmbedding(\n",
        "            args.max_source_positions + 1 + self.padding_idx, embed_dim, self.padding_idx,\n",
        "        )\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        self.layers.extend([\n",
        "            TransformerEncoderLayer(\n",
        "                args.encoder_embed_dim,\n",
        "                args.encoder_ffn_embed_dim,\n",
        "                args.encoder_attention_heads,\n",
        "                args.dropout,\n",
        "                args.attention_dropout,\n",
        "                args.activation_dropout,\n",
        "                args.activation_fn,\n",
        "            )\n",
        "            for i in range(args.encoder_layers)\n",
        "        ])\n",
        "\n",
        "        self.emb_layer_norm = LayerNorm(embed_dim)\n",
        "\n",
        "        self.apply(init_bert_params)\n",
        "\n",
        "    def forward(self, src_tokens, src_lengths, **unused):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            src_tokens (LongTensor): tokens in the source language of shape\n",
        "                `(batch, src_len)`\n",
        "            src_lengths (torch.LongTensor): lengths of each source sentence of\n",
        "                shape `(batch)`\n",
        "        Returns:\n",
        "            dict:\n",
        "                - **encoder_out** (Tensor): the last encoder layer's output of\n",
        "                  shape `(src_len, batch, embed_dim)`\n",
        "                - **encoder_padding_mask** (ByteTensor): the positions of\n",
        "                  padding elements of shape `(batch, src_len)`\n",
        "        \"\"\"\n",
        "        # compute padding mask\n",
        "        encoder_padding_mask = src_tokens.eq(self.padding_idx)\n",
        "        if not encoder_padding_mask.any():\n",
        "            encoder_padding_mask = None\n",
        "\n",
        "        x = self.embed_tokens(src_tokens)\n",
        "        # embed tokens and positions\n",
        "        if self.embed_scale is not None:\n",
        "            x *= self.embed_scale\n",
        "\n",
        "        if self.embed_positions is not None:\n",
        "            pos_emb, real_positions = self.embed_positions(src_tokens)\n",
        "            x += pos_emb\n",
        "\n",
        "        if self.emb_layer_norm:\n",
        "            x = self.emb_layer_norm(x)\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        if encoder_padding_mask is not None:\n",
        "            x *= 1 - encoder_padding_mask.unsqueeze(-1).type_as(x)\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "\n",
        "        # encoder layers\n",
        "        for layer in self.layers:\n",
        "            # x, _ = layer(x, self_attn_padding_mask=encoder_padding_mask, real_positions=real_positions)\n",
        "            x, _ = layer(x, self_attn_padding_mask=encoder_padding_mask,)\n",
        "\n",
        "        return {\n",
        "            'encoder_out': x,  # T x B x C\n",
        "            'encoder_padding_mask': encoder_padding_mask,  # B x T\n",
        "        }\n",
        "\n",
        "    def reorder_encoder_out(self, encoder_out, new_order):\n",
        "        \"\"\"\n",
        "        Reorder encoder output according to *new_order*.\n",
        "        Args:\n",
        "            encoder_out: output from the ``forward()`` method\n",
        "            new_order (LongTensor): desired order\n",
        "        Returns:\n",
        "            *encoder_out* rearranged according to *new_order*\n",
        "        \"\"\"\n",
        "        if encoder_out['encoder_out'] is not None:\n",
        "            encoder_out['encoder_out'] = \\\n",
        "                encoder_out['encoder_out'].index_select(1, new_order)\n",
        "        if encoder_out['encoder_padding_mask'] is not None:\n",
        "            encoder_out['encoder_padding_mask'] = \\\n",
        "                encoder_out['encoder_padding_mask'].index_select(0, new_order)\n",
        "        return encoder_out\n",
        "\n",
        "    def max_positions(self):\n",
        "        \"\"\"Maximum input length supported by the encoder.\"\"\"\n",
        "        if self.embed_positions is None:\n",
        "            return self.max_source_positions\n",
        "        return min(self.max_source_positions, self.embed_positions.max_positions())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_0gm9qTbA7f",
        "colab_type": "text"
      },
      "source": [
        "# Decoder Classes\n",
        "\n",
        "This includes the class for 1 layer and the class with multiple layers combined. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug_bBzO0Z7jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# decoder classes (layer and total)\n",
        "\n",
        "class NgramTransformerDecoderLayer(nn.Module):\n",
        "    def __init__(\n",
        "            self,\n",
        "            ngram=2,\n",
        "            embedding_dim: float = 768,\n",
        "            ffn_embedding_dim: float = 3072,\n",
        "            num_attention_heads: float = 8,\n",
        "            dropout: float = 0.1,\n",
        "            attention_dropout: float = 0.1,\n",
        "            activation_dropout: float = 0.1,\n",
        "            activation_fn: str = 'relu',\n",
        "            add_bias_kv: bool = False,\n",
        "            add_zero_attn: bool = False,\n",
        "            export: bool = False,\n",
        "\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.dropout = dropout\n",
        "        self.activation_dropout = activation_dropout\n",
        "\n",
        "        # Initialize blocks\n",
        "        self.activation_fn = utils.get_activation_fn(activation_fn)\n",
        "        self.ngram_self_attn = NgramMultiheadAttention(\n",
        "            self.embedding_dim,\n",
        "            num_attention_heads,\n",
        "            dropout=attention_dropout,\n",
        "            add_bias_kv=add_bias_kv,\n",
        "            add_zero_attn=add_zero_attn,\n",
        "            self_attention=True,\n",
        "            ngram=ngram\n",
        "        )\n",
        "        self.ngram = ngram\n",
        "\n",
        "        # layer norm associated with the self attention layer\n",
        "        self.self_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)\n",
        "\n",
        "        self.encoder_attn = MultiheadAttention(\n",
        "            self.embedding_dim,\n",
        "            num_attention_heads,\n",
        "            kdim=embedding_dim,\n",
        "            vdim=embedding_dim,\n",
        "            dropout=attention_dropout,\n",
        "            encoder_decoder_attention=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = LayerNorm(self.embedding_dim, export=export)\n",
        "\n",
        "        self.fc1 = nn.Linear(self.embedding_dim, ffn_embedding_dim)\n",
        "        self.fc2 = nn.Linear(ffn_embedding_dim, self.embedding_dim)\n",
        "\n",
        "        # layer norm associated with the position wise feed-forward NN\n",
        "        self.final_layer_norm = LayerNorm(self.embedding_dim, export=export)\n",
        "        self.need_attn = False\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            x,\n",
        "            encoder_out=None,\n",
        "            encoder_mask=None,\n",
        "            incremental_state=None,\n",
        "            prev_self_attn_state=None,\n",
        "            prev_attn_state=None,\n",
        "            self_attn_mask=None,\n",
        "            ngram_mask_matrix=None,\n",
        "            i_buckets_main_stream=None,\n",
        "            i_bucket_relative_stream=None,\n",
        "            real_positions=None\n",
        "    ):\n",
        "        # one main stream and ngram predicting streams\n",
        "        residual = x\n",
        "\n",
        "        if prev_self_attn_state is not None:\n",
        "            if incremental_state is None:\n",
        "                incremental_state = {}\n",
        "            prev_key, prev_value = prev_self_attn_state\n",
        "            saved_state = {\"prev_key\": prev_key, \"prev_value\": prev_value}\n",
        "            self.self_attn._set_input_buffer(incremental_state, saved_state)\n",
        "\n",
        "        x, attn = self.ngram_self_attn(\n",
        "            query=x,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            incremental_state=incremental_state,\n",
        "            need_weights=False,\n",
        "            self_attn_mask=self_attn_mask,\n",
        "            ngram_mask_matrix=ngram_mask_matrix,\n",
        "            i_buckets_main_stream=i_buckets_main_stream,\n",
        "            i_bucket_relative_stream=i_bucket_relative_stream,\n",
        "            real_positions=real_positions\n",
        "        )\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.self_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        if prev_attn_state is not None:\n",
        "            if incremental_state is None:\n",
        "                incremental_state = {}\n",
        "            prev_key, prev_value = prev_attn_state\n",
        "            saved_state = {\"prev_key\": prev_key, \"prev_value\": prev_value}\n",
        "            self.encoder_attn._set_input_buffer(incremental_state, saved_state)\n",
        "        x, attn = self.encoder_attn(\n",
        "            query=x,\n",
        "            key=encoder_out,\n",
        "            value=encoder_out,\n",
        "            key_padding_mask=encoder_mask,\n",
        "            incremental_state=incremental_state,\n",
        "            static_kv=True,\n",
        "            need_weights=(not self.training and self.need_attn),\n",
        "        )\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.encoder_attn_layer_norm(x)\n",
        "\n",
        "        residual = x\n",
        "        x = self.activation_fn(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.activation_dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = residual + x\n",
        "        x = self.final_layer_norm(x)\n",
        "        return x, attn\n",
        "\n",
        "    def make_generation_fast_(self, need_attn=False, **kwargs):\n",
        "        self.need_attn = need_attn\n",
        "\n",
        "\n",
        "class NgramTransformerDecoder(FairseqIncrementalDecoder):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *args.decoder_layers* layers. Each layer\n",
        "    is a :class:`TransformerDecoderLayer`.\n",
        "    Args:\n",
        "        args (argparse.Namespace): parsed command-line arguments\n",
        "        dictionary (~fairseq.data.Dictionary): decoding dictionary\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "        no_encoder_attn (bool, optional): whether to attend to encoder outputs\n",
        "            (default: False).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, args, dictionary, embed_tokens, no_encoder_attn=False):\n",
        "        super().__init__(dictionary)\n",
        "        self.register_buffer('version', torch.Tensor([3]))\n",
        "        self.ngram = args.ngram\n",
        "        self.num_buckets = args.num_buckets\n",
        "        self.relative_max_distance = args.relative_max_distance\n",
        "\n",
        "        self.dropout = args.dropout\n",
        "        self.share_input_output_embed = args.share_decoder_input_output_embed\n",
        "\n",
        "        input_embed_dim = embed_tokens.embedding_dim\n",
        "        embed_dim = args.decoder_embed_dim\n",
        "\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_target_positions = args.max_target_positions\n",
        "        self.embed_dim = embed_dim\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_scale = None #math.sqrt(embed_dim)  # todo: try with input_embed_dim\n",
        "\n",
        "        self.embed_positions = LearnedPositionalEmbedding(\n",
        "            args.max_target_positions + 2 + self.padding_idx, embed_dim, self.padding_idx,\n",
        "        )\n",
        "\n",
        "        self.ngram_input_embed = Embedding(self.ngram, input_embed_dim, None)\n",
        "\n",
        "        self.layers = nn.ModuleList([])\n",
        "\n",
        "        self.layers.extend([\n",
        "            NgramTransformerDecoderLayer(\n",
        "                args.ngram,\n",
        "                args.decoder_embed_dim,\n",
        "                args.decoder_ffn_embed_dim,\n",
        "                args.decoder_attention_heads,\n",
        "                args.dropout,\n",
        "                args.attention_dropout,\n",
        "                args.activation_dropout,\n",
        "                args.activation_fn,\n",
        "\n",
        "            )\n",
        "            for _ in range(args.decoder_layers)\n",
        "        ])\n",
        "\n",
        "        if not self.share_input_output_embed:\n",
        "            self.embed_out = nn.Parameter(torch.Tensor(len(dictionary), self.embed_dim))\n",
        "            nn.init.normal_(self.embed_out, mean=0, std=self.embed_dim ** -0.5)\n",
        "\n",
        "        self.emb_layer_norm = LayerNorm(embed_dim)\n",
        "        self.apply(init_bert_params)\n",
        "\n",
        "    def forward(self,\n",
        "                prev_output_tokens,\n",
        "                encoder_out=None,\n",
        "                incremental_state=None,\n",
        "                **unused):\n",
        "        # T\n",
        "        T = prev_output_tokens.size(1)\n",
        "        # x [B, (1+ngram)*T, C]\n",
        "        x_list, extra = self.extract_features(prev_output_tokens, encoder_out, incremental_state, **unused)\n",
        "        x_predicted = x_list[1:]\n",
        "        x_predicted = [self.output_layer(x) for x in x_predicted]\n",
        "        if incremental_state is not None:\n",
        "            x_predicted = x_predicted[0]\n",
        "            for k in extra:\n",
        "                if extra[k] is not None:\n",
        "                    extra[k] = extra[k][0]\n",
        "        return x_predicted, extra\n",
        "\n",
        "    def _relative_positions_bucket(self, relative_positions, bidirectional=False):\n",
        "        num_buckets = self.num_buckets\n",
        "        max_distance = self.relative_max_distance\n",
        "        n = -relative_positions\n",
        "        result = 0\n",
        "        if bidirectional:\n",
        "            num_buckets = num_buckets // 2\n",
        "            result = result + torch.lt(n, torch.zeros_like(n)).int() * num_buckets\n",
        "            n = torch.abs(n)\n",
        "        else:\n",
        "            n = torch.max(n, torch.zeros_like(n))\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = torch.lt(n, max_exact)\n",
        "        val_if_large = max_exact + torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (\n",
        "                num_buckets - max_exact)\n",
        "        val_if_large = torch.min(val_if_large, torch.ones_like(val_if_large) * (num_buckets - 1))\n",
        "        val_if_large = val_if_large.int()\n",
        "        result = result + torch.where(is_small, n.int(), val_if_large)\n",
        "        return result\n",
        "\n",
        "    def cal_pretrain_relative_positions(self, real_positions):\n",
        "        # main stream\n",
        "        main_stream_relative_positions = real_positions.unsqueeze(1)\n",
        "        # [B,T,T/S]\n",
        "        main_stream_relative_positions = main_stream_relative_positions.repeat(1, real_positions.size(-1), 1)\n",
        "        # [B,T,1]\n",
        "        real_positions_main = real_positions.unsqueeze(-1)\n",
        "        main_stream_relative_positions = main_stream_relative_positions - real_positions_main\n",
        "\n",
        "        # predicting stream\n",
        "        # input shift\n",
        "        real_positions_shift_predicting_stream = real_positions - 1\n",
        "        # [B,1, 2*T]\n",
        "        predicting_stream_relative_positions = torch.cat((real_positions_shift_predicting_stream, real_positions),\n",
        "                                                         dim=-1).unsqueeze(1)\n",
        "        # [B,T, 2*T]\n",
        "        predicting_stream_relative_positions = predicting_stream_relative_positions.repeat(1, real_positions.size(-1),\n",
        "                                                                                           1)\n",
        "        # [B,T, 1]\n",
        "        real_positions_predicting_stream = real_positions.unsqueeze(-1)\n",
        "        predicting_stream_relative_positions = predicting_stream_relative_positions - real_positions_predicting_stream\n",
        "        i_buckets_main_stream = self._relative_positions_bucket(main_stream_relative_positions, bidirectional=False)\n",
        "        i_bucket_relative_stream = self._relative_positions_bucket(predicting_stream_relative_positions,\n",
        "                                                                   bidirectional=False)\n",
        "        return i_buckets_main_stream, i_bucket_relative_stream\n",
        "\n",
        "    def cal_finetune_relative_positions(self, real_positions):\n",
        "        n_tokens = real_positions.size(-1)\n",
        "        batch_size = real_positions.size(0)\n",
        "        if not hasattr(self,\n",
        "                       '_finetune_i_bucket_main_stream') or \\\n",
        "                self._finetune_i_bucket_main_stream is None \\\n",
        "                or self._finetune_i_bucket_main_stream.device != real_positions.device:\n",
        "            fake_positions = torch.arange(1, self.max_target_positions + 1).repeat(1, 1)\n",
        "            finetune_i_bucket_main_stream, finetune_i_bucket_predicting_stream = \\\n",
        "                self.cal_pretrain_relative_positions(fake_positions)\n",
        "            self._finetune_i_bucket_main_stream = finetune_i_bucket_main_stream.to(real_positions.device)\n",
        "            self._finetune_i_bucket_predicting_stream = finetune_i_bucket_predicting_stream.to(real_positions.device)\n",
        "        finetune_i_bucket_main_stream = self._finetune_i_bucket_main_stream[:, :n_tokens, :n_tokens].repeat(batch_size,\n",
        "                                                                                                            1, 1)\n",
        "        finetune_i_bucket_predicting_stream = torch.cat([\n",
        "            self._finetune_i_bucket_predicting_stream[:, :n_tokens, :n_tokens],\n",
        "            self._finetune_i_bucket_predicting_stream[:, :n_tokens,\n",
        "            self.max_target_positions:self.max_target_positions + n_tokens]\n",
        "        ], 2).repeat(batch_size, 1, 1)\n",
        "        return finetune_i_bucket_main_stream, finetune_i_bucket_predicting_stream\n",
        "\n",
        "    def extract_features(self, prev_output_tokens, encoder_out=None, incremental_state=None, **unused):\n",
        "        # embed positions\n",
        "        # [bos, A, B, C, D, eos] with real positions [1,2,3,4,5,6](main stream), [2,3,4,5,6,7](predicting stream)\n",
        "        # target [B,C,D] with prev [A,B,C] from [A,B,C,D] as pretraining span with real positions [2,3,4],\n",
        "        # but target actually [3,4,5] for fine tune with another [bos].\n",
        "        # thus [2,3,4] used for main stream shifted prev [A,B,C], [3,4,5] used for predicting [B,C,D]\n",
        "        if 'positions' in unused:\n",
        "            # pretrain procedure\n",
        "            main_stream_pos_embed = self.embed_positions._forward(unused['positions'])\n",
        "            real_positions = unused['positions']\n",
        "            i_buckets_main_stream, i_bucket_relative_stream = \\\n",
        "                self.cal_pretrain_relative_positions(real_positions)\n",
        "        else:\n",
        "            # fine tune procedure\n",
        "            main_stream_pos_embed, real_positions = self.embed_positions(\n",
        "                prev_output_tokens,\n",
        "                incremental_state=incremental_state,\n",
        "            ) if self.embed_positions is not None else None\n",
        "            if incremental_state is not None:\n",
        "                i_buckets_main_stream, i_bucket_relative_stream = None, None\n",
        "            else:\n",
        "                i_buckets_main_stream, i_bucket_relative_stream = \\\n",
        "                    self.cal_finetune_relative_positions(real_positions)\n",
        "\n",
        "        predicting_stream_pos_embed = self.embed_positions._forward(real_positions + 1)\n",
        "\n",
        "        if incremental_state is not None:\n",
        "            prev_output_tokens = prev_output_tokens[:, -1:]\n",
        "            if main_stream_pos_embed is not None:\n",
        "                main_stream_pos_embed = main_stream_pos_embed[:, -1:]\n",
        "\n",
        "        x = self.embed_tokens(prev_output_tokens)\n",
        "        # embed tokens and positions\n",
        "        if self.embed_scale is not None:\n",
        "            x *= self.embed_scale\n",
        "\n",
        "        if main_stream_pos_embed is not None:\n",
        "            x += main_stream_pos_embed\n",
        "\n",
        "        # B x T x C -> T x B x C\n",
        "        x = x.transpose(0, 1)\n",
        "        attn = None\n",
        "\n",
        "        inner_states = [x]\n",
        "        if main_stream_pos_embed is None:\n",
        "            print('positions should be used to predict ngrams')\n",
        "            raise Exception()\n",
        "\n",
        "        if self.embed_scale is not None:\n",
        "            ngram_input_embed = self.embed_scale * self.ngram_input_embed.weight\n",
        "        else:\n",
        "            ngram_input_embed = self.ngram_input_embed.weight\n",
        "\n",
        "        if incremental_state is not None:\n",
        "            B = x.size(1)\n",
        "            ngram_masks = [\n",
        "                (ngram_input_embed[ngram - 1] + predicting_stream_pos_embed).transpose(0, 1).repeat(1, B, 1)\n",
        "                for ngram in range(self.ngram)]\n",
        "        else:\n",
        "            ngram_masks = [(ngram_input_embed[ngram - 1] + predicting_stream_pos_embed).transpose(0, 1) for\n",
        "                           ngram in range(self.ngram)]\n",
        "\n",
        "        self_attn_mask = self.buffered_future_mask(x) if incremental_state is None else None\n",
        "        ngram_mask_matrix = self.buffered_future_mask_ngram(x) if incremental_state is None else None\n",
        "\n",
        "        # TODO in train [(1+ngram)*T, B, C], in inference [T+ngram, B, C]\n",
        "        x = torch.cat([x] + ngram_masks, 0)\n",
        "\n",
        "        if self.emb_layer_norm:\n",
        "            x = self.emb_layer_norm(x)\n",
        "\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        # decoder layers\n",
        "        for layer in self.layers:\n",
        "            x, attn = layer(\n",
        "                x,\n",
        "                encoder_out['encoder_out'] if encoder_out is not None else None,\n",
        "                encoder_out['encoder_padding_mask'] if encoder_out is not None else None,\n",
        "                incremental_state,\n",
        "                self_attn_mask=self_attn_mask,\n",
        "                ngram_mask_matrix=ngram_mask_matrix,\n",
        "                i_buckets_main_stream=i_buckets_main_stream,\n",
        "                i_bucket_relative_stream=i_bucket_relative_stream,\n",
        "                real_positions=real_positions\n",
        "            )\n",
        "            inner_states.append(x)\n",
        "\n",
        "        # TODO [(1+ngram)*T, B, C] -> [B, (1+ngram)*T, C]\n",
        "        x_list = x.transpose(0, 1).chunk(1 + self.ngram, 1)\n",
        "        if attn is not None:\n",
        "            attn_list = attn.transpose(0, 1).chunk(1 + self.ngram, 1)\n",
        "        else:\n",
        "            attn_list = None\n",
        "\n",
        "        return x_list, {'attn': attn_list}\n",
        "\n",
        "    def get_normalized_probs(self, net_output, log_probs, sample):\n",
        "        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n",
        "\n",
        "        if hasattr(self, 'adaptive_softmax') and self.adaptive_softmax is not None:\n",
        "            if sample is not None:\n",
        "                assert 'target' in sample\n",
        "                target = sample['target']\n",
        "            else:\n",
        "                target = None\n",
        "            out = self.adaptive_softmax.get_log_prob(net_output[0], target=target)\n",
        "            return out.exp_() if not log_probs else out\n",
        "        '''\n",
        "        logits_list = net_output[0]\n",
        "        if log_probs:\n",
        "            return [utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace) for logits in logits_list][0]\n",
        "        else:\n",
        "            return [utils.softmax(logits, dim=-1, onnx_trace=self.onnx_trace) for logits in logits_list][0]\n",
        "        '''\n",
        "        logits = net_output[0]\n",
        "        if log_probs:\n",
        "            return utils.log_softmax(logits, dim=-1, onnx_trace=self.onnx_trace)\n",
        "        else:\n",
        "            return utils.softmax(logits, dim=-1, onnx_trace=self.onnx_trace)\n",
        "        \n",
        "\n",
        "    def output_layer(self, features, **kwargs):\n",
        "        \"\"\"Project features to the vocabulary size.\"\"\"\n",
        "        # project back to size of vocabulary\n",
        "        if self.share_input_output_embed:\n",
        "            return F.linear(features, self.embed_tokens.weight)\n",
        "        else:\n",
        "            return F.linear(features, self.embed_out)\n",
        "\n",
        "    def max_positions(self):\n",
        "        \"\"\"Maximum output length supported by the decoder.\"\"\"\n",
        "        if self.embed_positions is None:\n",
        "            return self.max_target_positions\n",
        "        return min(self.max_target_positions, self.embed_positions.max_positions())\n",
        "\n",
        "    def buffered_future_mask(self, tensor):\n",
        "        dim = tensor.size(0)\n",
        "        if not hasattr(self,\n",
        "                       '_future_mask') or self._future_mask is None or self._future_mask.device != tensor.device or self._future_mask.size(\n",
        "            0) < dim:\n",
        "            self._future_mask = torch.triu(utils.fill_with_neg_inf(tensor.new(dim, dim)), 1)\n",
        "        return self._future_mask[:dim, :dim]\n",
        "\n",
        "    def buffered_future_mask_ngram(self, tensor):\n",
        "        dim = tensor.size(0)\n",
        "        if not hasattr(self,\n",
        "                       '_ngram_future_mask') or self._ngram_future_mask is None or self._ngram_future_mask.device != tensor.device:\n",
        "            self._ngram_future_mask = ngram_attention_bias(self.max_target_positions, self.ngram).type(tensor.dtype).to(\n",
        "                tensor.device)\n",
        "        ngram_future_mask = torch.cat([self._ngram_future_mask[:, :dim, :dim],\n",
        "                                       self._ngram_future_mask[:, :dim,\n",
        "                                       self.max_target_positions: self.max_target_positions + dim]\n",
        "                                       ], 2)\n",
        "        return ngram_future_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYsHL-02bJe_",
        "colab_type": "text"
      },
      "source": [
        "# Other Layers \n",
        "\n",
        "These layers are involved in the neural network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fhaHB6tZv4I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# embeddings layer\n",
        "\n",
        "def Embedding(num_embeddings, embedding_dim, padding_idx):\n",
        "    m = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
        "    nn.init.normal_(m.weight, mean=0, std=embedding_dim ** -0.5)\n",
        "    nn.init.constant_(m.weight[padding_idx], 0)\n",
        "    return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNgPLj3JZsJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# linear layer\n",
        "\n",
        "def Linear(in_features, out_features, bias=True):\n",
        "    m = nn.Linear(in_features, out_features, bias)\n",
        "    nn.init.xavier_uniform_(m.weight)\n",
        "    if bias:\n",
        "        nn.init.constant_(m.bias, 0.)\n",
        "    return m\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSS0Cg6_biVT",
        "colab_type": "text"
      },
      "source": [
        "# Multihead Attention Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnAMMcDahvCt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Copyright (c) 2017-present, Facebook, Inc.\n",
        "# All rights reserved.\n",
        "#\n",
        "# This source code is licensed under the license found in the LICENSE file in\n",
        "# the root directory of this source tree. An additional grant of patent rights\n",
        "# can be found in the PATENTS file in the same directory.\n",
        "\n",
        "def ngram_attention_bias(length, num_skip):\n",
        "        bias_result = []\n",
        "        for n_skip in range(num_skip):\n",
        "            bias_n_skip = []\n",
        "            for i in range(length):\n",
        "                bias_this = [float('-inf')] * (2 * length)\n",
        "                bias_this[length+i] = 0\n",
        "                first_k = i - n_skip\n",
        "                first_k = first_k if first_k > 0 else 0\n",
        "                for j in range(first_k+1):\n",
        "                    bias_this[j] = 0\n",
        "                bias_n_skip.append(bias_this)\n",
        "            bias_result.append(bias_n_skip)\n",
        "        return torch.from_numpy(np.array(bias_result, dtype=np.float32))\n",
        "\n",
        "\n",
        "class NgramMultiheadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention.\n",
        "\n",
        "    See \"Attention Is All You Need\" for more details.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim, num_heads, kdim=None, vdim=None, dropout=0., bias=True,\n",
        "                 add_bias_kv=False, add_zero_attn=False, self_attention=False,\n",
        "                 encoder_decoder_attention=False,ngram=2, num_buckets=32, relative_max_distance=128):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.kdim = kdim if kdim is not None else embed_dim\n",
        "        self.vdim = vdim if vdim is not None else embed_dim\n",
        "        self.qkv_same_dim = self.kdim == embed_dim and self.vdim == embed_dim\n",
        "\n",
        "        self.num_buckets = num_buckets\n",
        "        self.relative_max_distance = relative_max_distance\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        self.ngram = ngram\n",
        "\n",
        "        assert self.head_dim * num_heads == self.embed_dim, \"embed_dim must be divisible by num_heads\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "\n",
        "        self.self_attention = self_attention\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "\n",
        "        assert not self.self_attention or self.qkv_same_dim, 'Self-attention requires query, key and ' \\\n",
        "                                                             'value to be of the same size'\n",
        "\n",
        "        self.relative_linear = nn.Linear(embed_dim, num_buckets * num_heads)\n",
        "        if self.qkv_same_dim:\n",
        "            self.in_proj_weight = Parameter(torch.Tensor(3 * embed_dim, embed_dim))\n",
        "        else:\n",
        "            self.k_proj_weight = Parameter(torch.Tensor(embed_dim, self.kdim))\n",
        "            self.v_proj_weight = Parameter(torch.Tensor(embed_dim, self.vdim))\n",
        "            self.q_proj_weight = Parameter(torch.Tensor(embed_dim, embed_dim))\n",
        "\n",
        "        if bias:\n",
        "            self.in_proj_bias = Parameter(torch.Tensor(3 * embed_dim))\n",
        "        else:\n",
        "            self.register_parameter('in_proj_bias', None)\n",
        "\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "        if add_bias_kv:\n",
        "            self.bias_k = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "            self.bias_v = Parameter(torch.Tensor(1, 1, embed_dim))\n",
        "        else:\n",
        "            self.bias_k = self.bias_v = None\n",
        "\n",
        "        self.add_zero_attn = add_zero_attn\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "        self.onnx_trace = False\n",
        "\n",
        "    def prepare_for_onnx_export_(self):\n",
        "        self.onnx_trace = True\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        if self.qkv_same_dim:\n",
        "            nn.init.xavier_uniform_(self.in_proj_weight)\n",
        "        else:\n",
        "            nn.init.xavier_uniform_(self.k_proj_weight)\n",
        "            nn.init.xavier_uniform_(self.v_proj_weight)\n",
        "            nn.init.xavier_uniform_(self.q_proj_weight)\n",
        "\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        if self.in_proj_bias is not None:\n",
        "            nn.init.constant_(self.in_proj_bias, 0.)\n",
        "            nn.init.constant_(self.out_proj.bias, 0.)\n",
        "        if self.bias_k is not None:\n",
        "            nn.init.xavier_normal_(self.bias_k)\n",
        "        if self.bias_v is not None:\n",
        "            nn.init.xavier_normal_(self.bias_v)\n",
        "\n",
        "    def _relative_positions_bucket(self, relative_positions, bidirectional=False):\n",
        "        num_buckets = self.num_buckets\n",
        "        max_distance = self.relative_max_distance\n",
        "        n = -relative_positions\n",
        "        result = 0\n",
        "        if bidirectional:\n",
        "            num_buckets = num_buckets // 2\n",
        "            result = result + torch.lt(n, torch.zeros_like(n)).int() * num_buckets\n",
        "            n = torch.abs(n)\n",
        "        else:\n",
        "            n = torch.max(n, torch.zeros_like(n))\n",
        "        max_exact = num_buckets // 2\n",
        "        is_small = torch.lt(n, max_exact)\n",
        "        val_if_large = max_exact + torch.log(n.float() / max_exact) / math.log(max_distance / max_exact) * (\n",
        "                    num_buckets - max_exact)\n",
        "        val_if_large = torch.min(val_if_large, torch.ones_like(val_if_large) * (num_buckets - 1))\n",
        "        val_if_large = val_if_large.int()\n",
        "        result = result + torch.where(is_small, n.int(), val_if_large)\n",
        "        return result\n",
        "\n",
        "\n",
        "    def main_stream_relative_logits(self,query, attn_weights, real_positions,i_bucket_main_stream):\n",
        "        # input query [T,B,C]\n",
        "        # input attn_weights [T*head,T,S]\n",
        "        # input real_positions [B,T] or [1,1]\n",
        "\n",
        "        T,B,_ = query.size()\n",
        "        S = attn_weights.size(-1)\n",
        "\n",
        "        if i_bucket_main_stream is not None:\n",
        "            i_buckets = i_bucket_main_stream\n",
        "        else:\n",
        "            # [B,T,S]\n",
        "            relative_positions = torch.arange(1, S+1).unsqueeze(0).unsqueeze(0).repeat(B,T,1).to(real_positions.device)\n",
        "            # [B,T,1]\n",
        "            real_positions = real_positions.unsqueeze(0).repeat(B,T,1)\n",
        "            # [B,T,S]\n",
        "            relative_positions = relative_positions - real_positions\n",
        "            # [B,T,T]\n",
        "            i_buckets = self._relative_positions_bucket(relative_positions, False)\n",
        "\n",
        "        # [B,T,C]\n",
        "        query = query.transpose(0,1)\n",
        "        # [B,T,Buckets*head]\n",
        "        values = self.relative_linear(query)\n",
        "        # [B,T,Buckets,head]\n",
        "        values = values.view(values.size(0),values.size(1),self.num_buckets, self.num_heads)\n",
        "        # [B,head,Buckets,T]\n",
        "        values = values.transpose(1,3)\n",
        "        # [B,head,T,Buckets]\n",
        "        values = values.transpose(2,3)\n",
        "        # [B*head,T,Buckets]\n",
        "        values = values.reshape(attn_weights.size(0),attn_weights.size(1),-1)\n",
        "\n",
        "        # => [B,head*T,T] => [B*head,T,T]\n",
        "        i_buckets = i_buckets.repeat(1,self.num_heads,1).view(attn_weights.size(0),attn_weights.size(1),-1)\n",
        "        # [B*head*T,Buckets]\n",
        "        values = values.reshape(-1, values.size(-1))\n",
        "        # [B*head*T,T]\n",
        "        i_buckets = i_buckets.view(-1, i_buckets.size(-1)).long()\n",
        "        # [B*head*T,T]\n",
        "        result = torch.gather(values,dim=1,index=i_buckets)\n",
        "        # [B*head,T,T]\n",
        "        result = result.view(attn_weights.size(0),attn_weights.size(1),-1)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def ngram_relative_logits(self, query, attn_weights, real_positions, i_bucket_relative_stream):\n",
        "        # input query [ngram, T,B,C]\n",
        "        # input attn_weights [ngram, B*head,T,S]\n",
        "        # input real_positions [B,T] or [1,1]\n",
        "        # input i_bucket_relative_stream [B,T, 2*T] or None\n",
        "\n",
        "        N, T, B, _ = query.size()\n",
        "        _, BH, _, S = attn_weights.size()\n",
        "\n",
        "        if i_bucket_relative_stream is not None:\n",
        "            i_buckets = i_bucket_relative_stream\n",
        "        else:\n",
        "            # [B,T,S]\n",
        "            assert real_positions[0][0] == S - 1, 'memory position is 1 2 3 4 5(S-1)'\n",
        "            relative_positions = torch.arange(0, S).unsqueeze(0).unsqueeze(0).repeat(B,T,1).to(real_positions.device)\n",
        "            # print('relative_positions', relative_positions)\n",
        "            # [B,T,1]\n",
        "            real_positions = real_positions.unsqueeze(0).repeat(B,T,1)\n",
        "            relative_positions = relative_positions\n",
        "            # [B,T,2*T] or [B,T,S]\n",
        "            relative_positions = relative_positions - real_positions\n",
        "            i_buckets = self._relative_positions_bucket(relative_positions, False)\n",
        "\n",
        "        # [ngram, B, T, C]\n",
        "        query = query.transpose(1,2)\n",
        "        # [ngram, B, T, bucket*head]\n",
        "        values = self.relative_linear(query)\n",
        "        # [ngram, B, T, bucket, head]\n",
        "        values = values.view(*values.size()[:-1],self.num_buckets, self.num_heads)\n",
        "        # [ngram, B, head, T, bucket]\n",
        "        values = values.permute(0, 1, 4, 2, 3)\n",
        "        # [ngram*B*head, T, bucket]\n",
        "        values = values.reshape(N*BH,T,-1)\n",
        "\n",
        "        # [ngram, B, head*T, S]\n",
        "        i_buckets = i_buckets.unsqueeze(0).repeat(N,1,self.num_heads,1)\n",
        "\n",
        "        values = values.reshape(-1, values.size(-1))\n",
        "        i_buckets = i_buckets.view(-1, i_buckets.size(-1)).long()\n",
        "        # [ngram*B*head*T, S]\n",
        "        result = torch.gather(values,dim=1,index=i_buckets)\n",
        "        # [ngram, B*head, T, S]\n",
        "        result = result.view(N, BH , T, -1)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def forward(self, query, key, value, key_padding_mask=None, incremental_state=None,\n",
        "                need_weights=True, static_kv=False,\n",
        "                self_attn_mask=None,\n",
        "                ngram_mask_matrix=None,\n",
        "                i_buckets_main_stream=None,\n",
        "                i_bucket_relative_stream=None,\n",
        "                real_positions=None\n",
        "                ):\n",
        "        \"\"\"Input shape: Time x Batch x Channel\n",
        "\n",
        "        Timesteps can be masked by supplying a T x T mask in the\n",
        "        `attn_mask` argument. Padding elements can be excluded from\n",
        "        the key by passing a binary ByteTensor (`key_padding_mask`) with shape:\n",
        "        batch x src_len, where padding elements are indicated by 1s.\n",
        "        \"\"\"\n",
        "\n",
        "        tgt_len, bsz, embed_dim = query.size()\n",
        "        assert embed_dim == self.embed_dim\n",
        "        assert list(query.size()) == [tgt_len, bsz, embed_dim]\n",
        "\n",
        "        if incremental_state is not None:\n",
        "            saved_state = self._get_input_buffer(incremental_state)\n",
        "            if 'prev_key' in saved_state:\n",
        "                # previous time steps are cached - no need to recompute\n",
        "                # key and value if they are static\n",
        "                if static_kv:\n",
        "                    assert self.encoder_decoder_attention and not self.self_attention\n",
        "                    key = value = None\n",
        "        else:\n",
        "            saved_state = None\n",
        "\n",
        "        q, k, v = self.in_proj_qkv(query)\n",
        "        q *= self.scaling\n",
        "\n",
        "        if self.bias_k is not None:\n",
        "            assert self.bias_v is not None\n",
        "            k = torch.cat([k, self.bias_k.repeat(1, bsz, 1)])\n",
        "            v = torch.cat([v, self.bias_v.repeat(1, bsz, 1)])\n",
        "        q = q.contiguous().view(tgt_len, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        if k is not None:\n",
        "            k = k.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "        if v is not None:\n",
        "            v = v.contiguous().view(-1, bsz * self.num_heads, self.head_dim).transpose(0, 1)\n",
        "\n",
        "        # h: \n",
        "        h_list = query.chunk(1 + self.ngram, dim=0)\n",
        "\n",
        "        q_list = q.chunk(1 + self.ngram, dim=1)\n",
        "        k_list = k.chunk(1 + self.ngram, dim=1)\n",
        "        v_list = v.chunk(1 + self.ngram, dim=1)\n",
        "\n",
        "        h_main, h_predict_list = h_list[0], h_list[1:]\n",
        "        q_main, q_predict_list = q_list[0], q_list[1:]\n",
        "        k_main, k_predict_list = k_list[0], k_list[1:]\n",
        "        v_main, v_predict_list = v_list[0], v_list[1:]\n",
        "\n",
        "        if saved_state is not None:\n",
        "            # saved states are stored with shape (bsz, num_heads, seq_len, head_dim)\n",
        "            if 'prev_key' in saved_state:\n",
        "                prev_key = saved_state['prev_key'].view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    assert False, 'static_kv not supprt in ngram decoder'\n",
        "                    k = prev_key\n",
        "                else:\n",
        "                    k_main = torch.cat((prev_key, k_main), dim=1)\n",
        "            if 'prev_value' in saved_state:\n",
        "                prev_value = saved_state['prev_value'].view(bsz * self.num_heads, -1, self.head_dim)\n",
        "                if static_kv:\n",
        "                    v = prev_value\n",
        "                else:\n",
        "                    v_main = torch.cat((prev_value, v_main), dim=1)\n",
        "            saved_state['prev_key'] = k_main.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "            saved_state['prev_value'] = v_main.view(bsz, self.num_heads, -1, self.head_dim)\n",
        "\n",
        "            self._set_input_buffer(incremental_state, saved_state)\n",
        "\n",
        "        real_tgt_len = tgt_len // (1 + self.ngram)\n",
        "\n",
        "        attn_weights_main = torch.bmm(q_main, k_main.transpose(1, 2))\n",
        "\n",
        "        main_relative_logits = self.main_stream_relative_logits(h_main, attn_weights_main,real_positions, i_buckets_main_stream)\n",
        "        attn_weights_main = attn_weights_main + main_relative_logits\n",
        "\n",
        "        if self_attn_mask is not None:\n",
        "            self_attn_mask = self_attn_mask.unsqueeze(0)\n",
        "            attn_weights_main = attn_weights_main + self_attn_mask\n",
        "\n",
        "        attn_weights_main = utils.softmax(\n",
        "            attn_weights_main, dim=-1, onnx_trace=self.onnx_trace,\n",
        "        ).type_as(attn_weights_main)\n",
        "        attn_weights_main = F.dropout(attn_weights_main, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_main = torch.bmm(attn_weights_main, v_main)\n",
        "        attn_main = attn_main.transpose(0, 1).contiguous().view(1, real_tgt_len, bsz, embed_dim)\n",
        "        attn_main = self.out_proj(attn_main)\n",
        "\n",
        "\n",
        "        # [ngram, B*head, T, c]\n",
        "        q_ngram = torch.cat(q_predict_list, 0).view(self.ngram, -1, real_tgt_len, self.head_dim)\n",
        "        # [ngram, B*head, 2*T, c]\n",
        "        k_ngram = torch.cat([torch.cat([k_main, k_p], 1).unsqueeze(0) for k_p in k_predict_list], 0)\n",
        "        # below code slower than above for loop\n",
        "        # k_ngram = torch.cat([k_main.unsqueeze(0).repeat(self.ngram, 1, 1, 1) , torch.cat(k_predict_list).view(self.ngram, -1, real_tgt_len, self.head_dim)], 2)\n",
        "\n",
        "        # [ngram, T, B, C]\n",
        "        h_ngram = torch.cat(h_predict_list, 0).view(self.ngram, real_tgt_len, bsz, embed_dim)\n",
        "\n",
        "        # [ngram, B*head, 2*T, c]\n",
        "        v_ngram = torch.cat([torch.cat([v_main, v_p], 1).unsqueeze(0) for v_p in v_predict_list], 0)\n",
        "        # below code slower than above for loop\n",
        "        # v_ngram = torch.cat([v_main.unsqueeze(0).repeat(self.ngram, 1, 1, 1) , torch.cat(v_predict_list).view(self.ngram, -1, real_tgt_len, self.head_dim)], 2)\n",
        "\n",
        "        # [ngram, B*head, T, 2*T]\n",
        "        attn_weights_ngram = torch.einsum('nbtc,nbsc->nbts', (q_ngram, k_ngram))\n",
        "\n",
        "        # [ngram, B*head, T, S]\n",
        "        predict_relative_logits = self.ngram_relative_logits(h_ngram, attn_weights_ngram, real_positions, i_bucket_relative_stream)\n",
        "        # [ngram, B*head, T, 2*T]\n",
        "        attn_weights_ngram = attn_weights_ngram + predict_relative_logits\n",
        "\n",
        "        if ngram_mask_matrix is not None:\n",
        "            ngram_mask_matrix = ngram_mask_matrix.unsqueeze(1)\n",
        "            attn_weights_ngram = attn_weights_ngram + ngram_mask_matrix\n",
        "\n",
        "        attn_weights_ngram = utils.softmax(\n",
        "            attn_weights_ngram, dim=-1, onnx_trace=self.onnx_trace,\n",
        "        ).type_as(attn_weights_ngram)\n",
        "        attn_weights_ngram = F.dropout(attn_weights_ngram, p=self.dropout, training=self.training)\n",
        "\n",
        "        # [ngram, B*head, T, c]\n",
        "        attn_ngram = torch.einsum('nbts,nbsc->nbtc', (attn_weights_ngram, v_ngram))\n",
        "        # [ngram, T, B, C]\n",
        "        attn_ngram = attn_ngram.transpose(1, 2).contiguous().view(self.ngram, real_tgt_len, bsz, embed_dim)\n",
        "        attn_ngram = self.out_proj(attn_ngram)\n",
        "\n",
        "        attn_result = []\n",
        "        attn_result.append(attn_main)\n",
        "        attn_result.append(attn_ngram)\n",
        "\n",
        "        # [1+ngram*T, B, C]\n",
        "        attn = torch.cat(attn_result, 0).view(-1, bsz, embed_dim)\n",
        "        return attn, None\n",
        "\n",
        "    def in_proj_qkv(self, query):\n",
        "        return self._in_proj(query).chunk(3, dim=-1)\n",
        "\n",
        "    def in_proj_q(self, query):\n",
        "        if self.qkv_same_dim:\n",
        "            return self._in_proj(query, end=self.embed_dim)\n",
        "        else:\n",
        "            bias = self.in_proj_bias\n",
        "            if bias is not None:\n",
        "                bias = bias[:self.embed_dim]\n",
        "            return F.linear(query, self.q_proj_weight, bias)\n",
        "\n",
        "    def in_proj_k(self, key):\n",
        "        if self.qkv_same_dim:\n",
        "            return self._in_proj(key, start=self.embed_dim, end=2 * self.embed_dim)\n",
        "        else:\n",
        "            weight = self.k_proj_weight\n",
        "            bias = self.in_proj_bias\n",
        "            if bias is not None:\n",
        "                bias = bias[self.embed_dim:2 * self.embed_dim]\n",
        "            return F.linear(key, weight, bias)\n",
        "\n",
        "    def in_proj_v(self, value):\n",
        "        if self.qkv_same_dim:\n",
        "            return self._in_proj(value, start=2 * self.embed_dim)\n",
        "        else:\n",
        "            weight = self.v_proj_weight\n",
        "            bias = self.in_proj_bias\n",
        "            if bias is not None:\n",
        "                bias = bias[2 * self.embed_dim:]\n",
        "            return F.linear(value, weight, bias)\n",
        "\n",
        "    def _in_proj(self, input, start=0, end=None):\n",
        "        weight = self.in_proj_weight\n",
        "        bias = self.in_proj_bias\n",
        "        weight = weight[start:end, :]\n",
        "        if bias is not None:\n",
        "            bias = bias[start:end]\n",
        "        return F.linear(input, weight, bias)\n",
        "\n",
        "    def reorder_incremental_state(self, incremental_state, new_order):\n",
        "        \"\"\"Reorder buffered internal state (for incremental generation).\"\"\"\n",
        "        input_buffer = self._get_input_buffer(incremental_state)\n",
        "        if input_buffer is not None:\n",
        "            for k in input_buffer.keys():\n",
        "                input_buffer[k] = input_buffer[k].index_select(0, new_order)\n",
        "            self._set_input_buffer(incremental_state, input_buffer)\n",
        "\n",
        "    def _get_input_buffer(self, incremental_state):\n",
        "        return utils.get_incremental_state(\n",
        "            self,\n",
        "            incremental_state,\n",
        "            'attn_state',\n",
        "        ) or {}\n",
        "\n",
        "    def _set_input_buffer(self, incremental_state, buffer):\n",
        "        utils.set_incremental_state(\n",
        "            self,\n",
        "            incremental_state,\n",
        "            'attn_state',\n",
        "            buffer,\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfmUs38CWQdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DEFAULT_MAX_SOURCE_POSITIONS = 512\n",
        "DEFAULT_MAX_TARGET_POSITIONS = 512\n",
        "\n",
        "\n",
        "@register_model('ngram_transformer_prophet')\n",
        "class NgramTransformerProphetModel(FairseqEncoderDecoderModel):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        encoder (TransformerEncoder): the encoder\n",
        "        decoder (TransformerDecoder): the decoder\n",
        "    The Transformer model provides the following named architectures and\n",
        "    command-line arguments:\n",
        "    .. argparse::\n",
        "        :ref: fairseq.models.transformer_parser\n",
        "        :prog:\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__(encoder, decoder)\n",
        "\n",
        "    @staticmethod\n",
        "    def add_args(parser):\n",
        "        \"\"\"Add model-specific arguments to the parser.\"\"\"\n",
        "        parser.add_argument('--ngram', type=int, metavar='N',\n",
        "                            help='num of predicting grams')\n",
        "        parser.add_argument('--num_buckets', type=int, metavar='N',\n",
        "                            help='num of buckets for relative position')\n",
        "        parser.add_argument('--relative_max_distance', type=int, metavar='N',\n",
        "                            help='num of bucket for relative position')\n",
        "        # fmt: off\n",
        "        parser.add_argument('--activation-fn',\n",
        "                            choices=utils.get_available_activation_fns(),\n",
        "                            help='activation function to use')\n",
        "        parser.add_argument('--dropout', type=float, metavar='D',\n",
        "                            help='dropout probability')\n",
        "        parser.add_argument('--attention-dropout', type=float, metavar='D',\n",
        "                            help='dropout probability for attention weights')\n",
        "        parser.add_argument('--activation-dropout', type=float, metavar='D',\n",
        "                            help='dropout probability after activation in FFN.')\n",
        "\n",
        "        parser.add_argument('--encoder-embed-dim', type=int, metavar='N',\n",
        "                            help='encoder embedding dimension')\n",
        "        parser.add_argument('--encoder-ffn-embed-dim', type=int, metavar='N',\n",
        "                            help='encoder embedding dimension for FFN')\n",
        "        parser.add_argument('--encoder-layers', type=int, metavar='N',\n",
        "                            help='num encoder layers')\n",
        "        parser.add_argument('--encoder-attention-heads', type=int, metavar='N',\n",
        "                            help='num encoder attention heads')\n",
        "\n",
        "        parser.add_argument('--decoder-embed-dim', type=int, metavar='N',\n",
        "                            help='decoder embedding dimension')\n",
        "        parser.add_argument('--decoder-ffn-embed-dim', type=int, metavar='N',\n",
        "                            help='decoder embedding dimension for FFN')\n",
        "        parser.add_argument('--decoder-layers', type=int, metavar='N',\n",
        "                            help='num decoder layers')\n",
        "        parser.add_argument('--decoder-attention-heads', type=int, metavar='N',\n",
        "                            help='num decoder attention heads')\n",
        "\n",
        "        parser.add_argument('--share-all-embeddings', action='store_true',\n",
        "                            help='share encoder, decoder and output embeddings'\n",
        "                                 ' (requires shared dictionary and embed dim)')\n",
        "        parser.add_argument('--load-from-pretrained-model', type=str, default=None,\n",
        "                            help='Load from pretrained model')\n",
        "        parser.add_argument('--load-sep', action='store_true',\n",
        "                            help='load pretrained [SEP] weight into [X_SEP]. ([SEP] used as eos in fine tuning)')\n",
        "        # fmt: on\n",
        "\n",
        "    def get_normalized_probs(self, net_output, log_probs, sample=None):\n",
        "        \"\"\"Get normalized probabilities (or log probs) from a net's output.\"\"\"\n",
        "        if hasattr(self, 'decoder'):\n",
        "            return self.decoder.get_normalized_probs(net_output, log_probs, sample)\n",
        "        elif torch.is_tensor(net_output):\n",
        "            logits = net_output.float()\n",
        "            if log_probs:\n",
        "                return F.log_softmax(logits, dim=-1)\n",
        "            else:\n",
        "                return F.softmax(logits, dim=-1)\n",
        "        raise NotImplementedError\n",
        "\n",
        "    @classmethod\n",
        "    def build_model(cls, args, task):\n",
        "        \"\"\"Build a new model instance.\"\"\"\n",
        "\n",
        "        # make sure all arguments are present in older models\n",
        "        base_architecture(args)\n",
        "\n",
        "        if not hasattr(args, 'max_source_positions'):\n",
        "            args.max_source_positions = DEFAULT_MAX_SOURCE_POSITIONS\n",
        "        if not hasattr(args, 'max_target_positions'):\n",
        "            args.max_target_positions = DEFAULT_MAX_TARGET_POSITIONS\n",
        "\n",
        "        src_dict, tgt_dict = task.source_dictionary, task.target_dictionary\n",
        "\n",
        "        def build_embedding(dictionary, embed_dim):\n",
        "            num_embeddings = len(dictionary)\n",
        "            padding_idx = dictionary.pad()\n",
        "            emb = Embedding(num_embeddings, embed_dim, padding_idx)\n",
        "            return emb\n",
        "\n",
        "        if args.share_all_embeddings:\n",
        "            if src_dict != tgt_dict:\n",
        "                raise ValueError('--share-all-embeddings requires a joined dictionary')\n",
        "            if args.encoder_embed_dim != args.decoder_embed_dim:\n",
        "                raise ValueError(\n",
        "                    '--share-all-embeddings requires --encoder-embed-dim to match --decoder-embed-dim')\n",
        "            encoder_embed_tokens = build_embedding(\n",
        "                src_dict, args.encoder_embed_dim\n",
        "            )\n",
        "            decoder_embed_tokens = encoder_embed_tokens\n",
        "            args.share_decoder_input_output_embed = True\n",
        "        else:\n",
        "            encoder_embed_tokens = build_embedding(\n",
        "                src_dict, args.encoder_embed_dim\n",
        "            )\n",
        "            decoder_embed_tokens = build_embedding(\n",
        "                tgt_dict, args.decoder_embed_dim\n",
        "            )\n",
        "\n",
        "        encoder = TransformerEncoder(args, src_dict, encoder_embed_tokens)\n",
        "        decoder = NgramTransformerDecoder(args, tgt_dict, decoder_embed_tokens)\n",
        "\n",
        "        model = NgramTransformerProphetModel(encoder, decoder)\n",
        "\n",
        "        if args.load_from_pretrained_model is not None:\n",
        "            states = torch.load(args.load_from_pretrained_model, map_location='cpu')\n",
        "            if 'model' in states and 'args' in states:\n",
        "                states = states['model']\n",
        "            if args.load_sep:\n",
        "                encoder_token_weight = states['encoder.embed_tokens.weight']\n",
        "                decoder_token_weight = states['decoder.embed_tokens.weight']\n",
        "                encoder_token_weight[2] = encoder_token_weight[102]\n",
        "                decoder_token_weight[2] = decoder_token_weight[102]\n",
        "                states['encoder.embed_tokens.weight'] = encoder_token_weight\n",
        "                states['decoder.embed_tokens.weight'] = decoder_token_weight\n",
        "            for position_name, target_position_length in [('encoder.embed_positions.weight', model.encoder.embed_positions.weight.size(0)), \\\n",
        "                    ('decoder.embed_positions.weight', model.decoder.embed_positions.weight.size(0))]:\n",
        "                if states[position_name].size(0) < target_position_length:\n",
        "                    _index = torch.arange(states[position_name].size(1))\n",
        "                    expend_position_states = states[position_name].clone()\n",
        "                    while states[position_name].size(0) < target_position_length:\n",
        "                        _index = torch.cat((_index[1:],_index[:1]), dim=0)\n",
        "                        states[position_name] = torch.cat([states[position_name], expend_position_states[:,_index]], dim=0)\n",
        "                if states[position_name].size(0) > target_position_length:\n",
        "                    states[position_name] = states[position_name][:target_position_length]\n",
        "            model.load_state_dict(states)\n",
        "            args.load_from_pretrained_model = None  # Clear this param\n",
        "\n",
        "        return NgramTransformerProphetModel(encoder, decoder)\n",
        "\n",
        "    def max_positions(self):\n",
        "        return (self.encoder.max_positions(), self.decoder.max_positions())\n",
        "\n",
        "    def forward(self, src_tokens=None, src_lengths=None, prev_output_tokens=None, **kwargs):\n",
        "        \"\"\"\n",
        "        Run the forward pass for an encoder-decoder model.\n",
        "        First feed a batch of source tokens through the encoder. Then, feed the\n",
        "        encoder output and previous decoder outputs (i.e., teacher forcing) to\n",
        "        the decoder to produce the next outputs::\n",
        "            encoder_out = self.encoder(src_tokens, src_lengths)\n",
        "            return self.decoder(prev_output_tokens, encoder_out)\n",
        "        Args:\n",
        "            src_tokens (LongTensor): tokens in the source language of shape\n",
        "                `(batch, src_len)`\n",
        "            src_lengths (LongTensor): source sentence lengths of shape `(batch)`\n",
        "            prev_output_tokens (LongTensor): previous decoder outputs of shape\n",
        "                `(batch, tgt_len)`, for teacher forcing\n",
        "        Returns:\n",
        "            tuple:\n",
        "                - the decoder's output of shape `(batch, tgt_len, vocab)`\n",
        "                - a dictionary with any model-specific outputs\n",
        "        \"\"\"\n",
        "        encoder_out = self.encoder(src_tokens, src_lengths=src_lengths, **kwargs)\n",
        "        decoder_out = self.decoder(prev_output_tokens, encoder_out=encoder_out, **kwargs)\n",
        "        return decoder_out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGxotE5sbSXE",
        "colab_type": "text"
      },
      "source": [
        "# Architectures"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASAeVnhhZnnl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# base architecture\n",
        "\n",
        "@register_model_architecture('ngram_transformer_prophet', 'ngram_transformer_prophet')\n",
        "def base_architecture(args):\n",
        "    args.ngram = getattr(args, 'ngram', 2)\n",
        "    args.num_buckets = getattr(args, 'num_buckets', 32)\n",
        "    args.relative_max_distance = getattr(args, 'relative_max_distance', 128)\n",
        "\n",
        "    args.activation_fn = getattr(args, 'activation_fn', 'relu')\n",
        "    args.dropout = getattr(args, 'dropout', 0.1)\n",
        "    args.attention_dropout = getattr(args, 'attention_dropout', 0.)\n",
        "    args.activation_dropout = getattr(args, 'activation_dropout', 0.)\n",
        "\n",
        "    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 512)\n",
        "    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 2048)\n",
        "    args.encoder_layers = getattr(args, 'encoder_layers', 6)\n",
        "    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 8)\n",
        "\n",
        "    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 512)\n",
        "    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 2048)\n",
        "    args.decoder_layers = getattr(args, 'decoder_layers', 6)\n",
        "    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 8)\n",
        "\n",
        "    args.share_decoder_input_output_embed = getattr(args, 'share_decoder_input_output_embed', False)\n",
        "    args.share_all_embeddings = getattr(args, 'share_all_embeddings', False)\n",
        "    args.load_sep = getattr(args, 'load_sep', False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_tG8dr6YsSU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# transformer architectures\n",
        "\n",
        "@register_model_architecture('ngram_transformer_prophet', 'ngram_transformer_prophet_base')\n",
        "def transformer_base(args):\n",
        "    args.ngram = getattr(args, 'ngram', 2)\n",
        "    args.num_buckets = getattr(args, 'num_buckets', 32)\n",
        "    args.relative_max_distance = getattr(args, 'relative_max_distance', 128)\n",
        "\n",
        "    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 768)\n",
        "    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 3072)\n",
        "    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 12)\n",
        "    args.encoder_layers = getattr(args, 'encoder_layers', 6)\n",
        "\n",
        "    args.dropout = getattr(args, 'dropout', 0.1)\n",
        "    args.attention_dropout = getattr(args, 'attention_dropout', 0.1)\n",
        "    args.activation_dropout = getattr(args, 'activation_dropout', 0.1)\n",
        "    args.activation_fn = getattr(args, 'activation_fn', 'gelu')\n",
        "\n",
        "    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 768)\n",
        "    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 3072)\n",
        "    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 12)\n",
        "    args.decoder_layers = getattr(args, 'decoder_layers', 6)\n",
        "\n",
        "    args.share_decoder_input_output_embed = getattr(args, 'share_decoder_input_output_embed', True)\n",
        "    args.share_all_embeddings = getattr(args, 'share_all_embeddings', True)\n",
        "    base_architecture(args)\n",
        "\n",
        "\n",
        "@register_model_architecture('ngram_transformer_prophet', 'ngram_transformer_prophet_middle')\n",
        "def transformer_middle(args):\n",
        "    args.ngram = getattr(args, 'ngram', 2)\n",
        "    args.num_buckets = getattr(args, 'num_buckets', 32)\n",
        "    args.relative_max_distance = getattr(args, 'relative_max_distance', 128)\n",
        "\n",
        "    args.encoder_embed_dim = getattr(args, 'encoder_embed_dim', 1024)\n",
        "    args.encoder_ffn_embed_dim = getattr(args, 'encoder_ffn_embed_dim', 4096)\n",
        "    args.encoder_attention_heads = getattr(args, 'encoder_attention_heads', 16)\n",
        "    args.encoder_layers = getattr(args, 'encoder_layers', 6)\n",
        "\n",
        "    args.decoder_embed_dim = getattr(args, 'decoder_embed_dim', 1024)\n",
        "    args.decoder_ffn_embed_dim = getattr(args, 'decoder_ffn_embed_dim', 4096)\n",
        "    args.decoder_attention_heads = getattr(args, 'decoder_attention_heads', 16)\n",
        "    args.decoder_layers = getattr(args, 'decoder_layers', 6)\n",
        "    transformer_base(args)\n",
        "\n",
        "\n",
        "@register_model_architecture('ngram_transformer_prophet', 'ngram_transformer_prophet_large')\n",
        "def transformer_big(args):\n",
        "    args.ngram = getattr(args, 'ngram', 2)\n",
        "    args.num_buckets = getattr(args, 'num_buckets', 32)\n",
        "    args.relative_max_distance = getattr(args, 'relative_max_distance', 128)\n",
        "\n",
        "    args.encoder_layers = getattr(args, 'encoder_layers', 12)\n",
        "    args.decoder_layers = getattr(args, 'decoder_layers', 12)\n",
        "    transformer_middle(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HqX3FQFiSir",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}